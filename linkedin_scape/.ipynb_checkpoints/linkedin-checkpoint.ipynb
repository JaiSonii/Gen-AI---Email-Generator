{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87ec46b6-75f3-4c4c-8310-173e68163009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "# Import ChromeOptions to set headless mode\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9129a868-6398-4968-9450-be36831e6abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a45c876-5b82-41db-be22-a49c823479e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_with_selenium(url : str) -> str | None:\n",
    "        \"\"\"\n",
    "        Uses headless Selenium with a generic waiting strategy.\n",
    "        \"\"\"\n",
    "        # Configure headless options ---\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless=new\") # Runs Chrome without a UI\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\") # Optional: Specify window size\n",
    "\n",
    "        # Initialize the driver with the new options\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "        html_content = None\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            print(\"Waiting for page to load in headless mode...\")\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                lambda d: d.execute_script(\"return document.readyState\") == 'complete'\n",
    "            )\n",
    "            time.sleep(2)\n",
    "            \n",
    "            print(\"Content loaded successfully.\")\n",
    "            html_content = driver.page_source\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(\"Timed out waiting for page to load.\")\n",
    "            html_content = driver.page_source\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during Selenium scraping: {e}\")\n",
    "        finally:\n",
    "            driver.quit()\n",
    "        \n",
    "        if html_content:\n",
    "            return html_content\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f46f934-0415-437a-84d9-d3b2d74d0c09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for page to load in headless mode...\n",
      "Content loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.linkedin.com/in/jai-soni-879764257/\"\n",
    "res = scrape_with_selenium(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a2634e0-5cd4-4c9e-aaf9-ac1ef28beb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "def fetch_profile_html_with_cookies(profile_url, cookies_file=\"cookies.json\"):\n",
    "    # Set Chrome to headless mode\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # Chrome 109+ headless mode\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    # Load cookies\n",
    "    with open(cookies_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        cookies = json.load(f)\n",
    "\n",
    "    # Visit LinkedIn first so domain matches\n",
    "    driver.get(\"https://www.linkedin.com\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Inject cookies\n",
    "    for cookie in cookies:\n",
    "        cookie_dict = {\n",
    "            \"name\": cookie.get(\"name\"),\n",
    "            \"value\": cookie.get(\"value\"),\n",
    "            \"domain\": cookie.get(\"domain\"),\n",
    "            \"path\": cookie.get(\"path\"),\n",
    "            \"secure\": cookie.get(\"secure\"),\n",
    "            \"httpOnly\": cookie.get(\"httpOnly\")\n",
    "        }\n",
    "        try:\n",
    "            driver.add_cookie(cookie_dict)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping cookie {cookie.get('name')} â†’ {e}\")\n",
    "\n",
    "    # Load profile page\n",
    "    driver.get(profile_url)\n",
    "    time.sleep(5)  # wait for full load\n",
    "\n",
    "    html_content = driver.page_source\n",
    "    with open(\"res.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html)\n",
    "    f.close()\n",
    "    driver.quit()\n",
    "    return html_content\n",
    "\n",
    "def parse_linkedin_profile_with_cookies(profile_url, cookies_file=\"cookies.json\"):\n",
    "    html_content = fetch_profile_html_with_cookies(profile_url, cookies_file)\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    profile = {\n",
    "        \"id\": None,\n",
    "        \"name\": None,\n",
    "        \"city\": None,\n",
    "        \"country_code\": None,\n",
    "        \"position\": None,\n",
    "        \"about\": None,\n",
    "        \"current_company\": {},\n",
    "        \"experience\": [],\n",
    "        \"url\": profile_url,\n",
    "        \"education\": [],\n",
    "        \"avatar\": None,\n",
    "        \"certifications\": [],\n",
    "        \"followers\": None,\n",
    "        \"connections\": None,\n",
    "        \"projects\": [],\n",
    "        \"location\": None,\n",
    "        \"activity\": []\n",
    "    }\n",
    "\n",
    "    # BASIC INFO\n",
    "    name_tag = soup.find(\"h1\", {\"class\": re.compile(\".*text-heading-xlarge.*\")})\n",
    "    if name_tag:\n",
    "        profile[\"name\"] = name_tag.get_text(strip=True)\n",
    "\n",
    "    headline_tag = soup.find(\"div\", {\"class\": re.compile(\".*text-body-medium.*\")})\n",
    "    if headline_tag:\n",
    "        profile[\"position\"] = headline_tag.get_text(strip=True)\n",
    "\n",
    "    location_tag = soup.find(\"span\", {\"class\": re.compile(\".*text-body-small.*\")})\n",
    "    if location_tag:\n",
    "        profile[\"location\"] = location_tag.get_text(strip=True)\n",
    "\n",
    "    avatar_tag = soup.find(\"img\", {\"class\": re.compile(\".*pv-top-card-profile-picture__image.*\")})\n",
    "    if avatar_tag and avatar_tag.get(\"src\"):\n",
    "        profile[\"avatar\"] = avatar_tag[\"src\"]\n",
    "\n",
    "    # ABOUT\n",
    "    about_section = soup.find(\"section\", {\"id\": \"about\"})\n",
    "    if about_section:\n",
    "        profile[\"about\"] = about_section.get_text(separator=\" \", strip=True)\n",
    "\n",
    "    # EXPERIENCE\n",
    "    experience_section = soup.find(\"section\", {\"id\": \"experience\"})\n",
    "    if experience_section:\n",
    "        roles = experience_section.find_all(\"li\")\n",
    "        for role in roles:\n",
    "            title_tag = role.find(\"span\", {\"class\": re.compile(\".*mr1.*\")})\n",
    "            company_tag = role.find(\"span\", {\"class\": re.compile(\".*t-14.*\")})\n",
    "            date_tag = role.find(\"span\", string=re.compile(r\"\\d{4}\"))\n",
    "            loc_tag = role.find(\"span\", {\"class\": re.compile(\".*t-14.*\")})\n",
    "\n",
    "            profile[\"experience\"].append({\n",
    "                \"title\": title_tag.get_text(strip=True) if title_tag else None,\n",
    "                \"company\": company_tag.get_text(strip=True) if company_tag else None,\n",
    "                \"start_date\": date_tag.get_text(strip=True) if date_tag else None,\n",
    "                \"location\": loc_tag.get_text(strip=True) if loc_tag else None\n",
    "            })\n",
    "\n",
    "    # EDUCATION\n",
    "    education_section = soup.find(\"section\", {\"id\": \"education\"})\n",
    "    if education_section:\n",
    "        schools = education_section.find_all(\"li\")\n",
    "        for school in schools:\n",
    "            school_name = school.find(\"span\", {\"class\": re.compile(\".*mr1.*\")})\n",
    "            degree = school.find(\"span\", string=re.compile(\"BTech|Bachelor|Master|B.Sc|M.Sc\"))\n",
    "            year_tag = school.find(\"span\", string=re.compile(r\"\\d{4}\"))\n",
    "\n",
    "            profile[\"education\"].append({\n",
    "                \"title\": school_name.get_text(strip=True) if school_name else None,\n",
    "                \"degree\": degree.get_text(strip=True) if degree else None,\n",
    "                \"start_year\": year_tag.get_text(strip=True) if year_tag else None\n",
    "            })\n",
    "\n",
    "    # FOLLOWERS & CONNECTIONS\n",
    "    followers_tag = soup.find(string=re.compile(r\"followers\"))\n",
    "    if followers_tag:\n",
    "        followers_count = re.findall(r\"\\d[\\d,]*\", followers_tag)\n",
    "        if followers_count:\n",
    "            profile[\"followers\"] = int(followers_count[0].replace(\",\", \"\"))\n",
    "\n",
    "    connections_tag = soup.find(string=re.compile(r\"connections\"))\n",
    "    if connections_tag:\n",
    "        connections_count = re.findall(r\"\\d[\\d,]*\", connections_tag)\n",
    "        if connections_count:\n",
    "            profile[\"connections\"] = int(connections_count[0].replace(\",\", \"\"))\n",
    "\n",
    "    return profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49c08eca-4b18-40b6-820f-403988447f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': None,\n",
       " 'name': None,\n",
       " 'city': None,\n",
       " 'country_code': None,\n",
       " 'position': 'Software Engineer @FOG Technologies | Full Stack Developer | AI Enthusiast',\n",
       " 'about': None,\n",
       " 'current_company': {},\n",
       " 'experience': [],\n",
       " 'url': 'https://www.linkedin.com/in/jai-soni-879764257/',\n",
       " 'education': [],\n",
       " 'avatar': 'https://media.licdn.com/dms/image/v2/D4D35AQHYv2na6-1-iA/profile-framedphoto-shrink_400_400/B4DZgSIZbzG8Ac-/0/1752650848394?e=1755446400&v=beta&t=FJTxRkhz7jPVyUYu5Yaj51wtUl2pbP2UG5EpVwp1fCg',\n",
       " 'certifications': [],\n",
       " 'followers': 4,\n",
       " 'connections': 95,\n",
       " 'projects': [],\n",
       " 'location': 'He/Him',\n",
       " 'activity': []}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_linkedin_profile_with_cookies(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66034fff-a0dc-4676-86dd-da646863144d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
