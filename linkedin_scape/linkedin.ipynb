{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87ec46b6-75f3-4c4c-8310-173e68163009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "# Import ChromeOptions to set headless mode\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9129a868-6398-4968-9450-be36831e6abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a45c876-5b82-41db-be22-a49c823479e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_with_selenium(url : str) -> str | None:\n",
    "        \"\"\"\n",
    "        Uses headless Selenium with a generic waiting strategy.\n",
    "        \"\"\"\n",
    "        # Configure headless options ---\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless=new\") # Runs Chrome without a UI\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\") # Optional: Specify window size\n",
    "\n",
    "        # Initialize the driver with the new options\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "        html_content = None\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            print(\"Waiting for page to load in headless mode...\")\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                lambda d: d.execute_script(\"return document.readyState\") == 'complete'\n",
    "            )\n",
    "            time.sleep(2)\n",
    "            \n",
    "            print(\"Content loaded successfully.\")\n",
    "            html_content = driver.page_source\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(\"Timed out waiting for page to load.\")\n",
    "            html_content = driver.page_source\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during Selenium scraping: {e}\")\n",
    "        finally:\n",
    "            driver.quit()\n",
    "        \n",
    "        if html_content:\n",
    "            return html_content\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f46f934-0415-437a-84d9-d3b2d74d0c09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for page to load in headless mode...\n",
      "Content loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.linkedin.com/in/jai-soni-879764257/\"\n",
    "res = scrape_with_selenium(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a2634e0-5cd4-4c9e-aaf9-ac1ef28beb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def fetch_profile_html_with_cookies(profile_url, cookies_file=\"cookies.json\"):\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    import json, time\n",
    "\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless=new\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    with open(cookies_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        cookies = json.load(f)\n",
    "\n",
    "    driver.get(\"https://www.linkedin.com\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    for cookie in cookies:\n",
    "        try:\n",
    "            driver.add_cookie({\n",
    "                \"name\": cookie.get(\"name\"),\n",
    "                \"value\": cookie.get(\"value\"),\n",
    "                \"domain\": cookie.get(\"domain\"),\n",
    "                \"path\": cookie.get(\"path\"),\n",
    "                \"secure\": cookie.get(\"secure\"),\n",
    "                \"httpOnly\": cookie.get(\"httpOnly\")\n",
    "            })\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    driver.get(profile_url)\n",
    "\n",
    "    # Wait for the name element\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"h1.text-heading-xlarge\"))\n",
    "        )\n",
    "    except:\n",
    "        print(\"⚠ Name element not found — possibly not logged in or restricted profile.\")\n",
    "\n",
    "    # Scroll slowly to load all sections\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    html_content = driver.page_source\n",
    "    driver.quit()\n",
    "    return html_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "296e654d-9ba6-40e5-8035-6238e9d44e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def parse_linkedin_profile(html_content: str, profile_url: str):\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    profile = {\n",
    "        \"id\": None,\n",
    "        \"name\": None,\n",
    "        \"city\": None,\n",
    "        \"country_code\": None,\n",
    "        \"position\": None,\n",
    "        \"about\": None,\n",
    "        \"current_company\": {\n",
    "            \"name\": None,\n",
    "            \"id\": None,\n",
    "            \"title\": None,\n",
    "            \"location\": None\n",
    "        },\n",
    "        \"experience\": [],\n",
    "        \"url\": profile_url,\n",
    "        \"education\": [],\n",
    "        \"avatar\": None,\n",
    "        \"certifications\": [],\n",
    "        \"followers\": None,\n",
    "        \"connections\": None,\n",
    "        \"projects\": [],\n",
    "        \"location\": None,\n",
    "        \"activity\": []\n",
    "    }\n",
    "\n",
    "    # ID from meta tag or URL\n",
    "    canonical_link = soup.find(\"link\", {\"rel\": \"canonical\"})\n",
    "    if canonical_link and \"linkedin.com/in/\" in canonical_link.get(\"href\", \"\"):\n",
    "        profile[\"id\"] = canonical_link[\"href\"].split(\"/\")[-2]\n",
    "    \n",
    "    # Name\n",
    "    name_tag = soup.find(\"h1\", {\"class\": re.compile(\".*text-heading-xlarge.*\")})\n",
    "    if name_tag:\n",
    "        profile[\"name\"] = name_tag.get_text(strip=True)\n",
    "\n",
    "    # Headline\n",
    "    headline_tag = soup.find(\"div\", {\"class\": re.compile(\".*text-body-medium.*\")})\n",
    "    if headline_tag:\n",
    "        profile[\"position\"] = headline_tag.get_text(strip=True)\n",
    "\n",
    "    # Location\n",
    "    location_tag = soup.find(\"span\", {\"class\": re.compile(\".*text-body-small.*\")})\n",
    "    if location_tag:\n",
    "        location_text = location_tag.get_text(strip=True)\n",
    "        profile[\"location\"] = location_text\n",
    "        parts = location_text.split(\",\")\n",
    "        if len(parts) >= 1:\n",
    "            profile[\"city\"] = parts[0].strip()\n",
    "        if len(parts) >= 2:\n",
    "            profile[\"country_code\"] = parts[-1].strip()\n",
    "\n",
    "    # Avatar\n",
    "    avatar_tag = soup.find(\"img\", {\"class\": re.compile(\".*pv-top-card-profile-picture__image.*\")})\n",
    "    if avatar_tag and avatar_tag.get(\"src\"):\n",
    "        profile[\"avatar\"] = avatar_tag[\"src\"]\n",
    "\n",
    "    # About\n",
    "    about_section = soup.find(\"section\", {\"id\": \"about\"})\n",
    "    if about_section:\n",
    "        profile[\"about\"] = about_section.get_text(separator=\" \", strip=True)\n",
    "\n",
    "    # Experience\n",
    "    experience_section = soup.find(\"section\", {\"id\": \"experience\"})\n",
    "    if experience_section:\n",
    "        for role in experience_section.find_all(\"li\", recursive=True):\n",
    "            title = role.find(\"span\", {\"class\": re.compile(\".*mr1.*\")})\n",
    "            company = role.find(\"span\", {\"class\": re.compile(\".*t-14.*\")})\n",
    "            date_range = role.find(string=re.compile(r\"\\d{4}\"))\n",
    "            loc = role.find(\"span\", {\"class\": re.compile(\".*t-14.*\")})\n",
    "            profile[\"experience\"].append({\n",
    "                \"title\": title.get_text(strip=True) if title else None,\n",
    "                \"company\": company.get_text(strip=True) if company else None,\n",
    "                \"start_date\": date_range.strip() if date_range else None,\n",
    "                \"location\": loc.get_text(strip=True) if loc else None\n",
    "            })\n",
    "\n",
    "    # Education\n",
    "    education_section = soup.find(\"section\", {\"id\": \"education\"})\n",
    "    if education_section:\n",
    "        for school in education_section.find_all(\"li\"):\n",
    "            school_name = school.find(\"span\", {\"class\": re.compile(\".*mr1.*\")})\n",
    "            degree = school.find(\"span\", string=re.compile(\"BTech|Bachelor|Master|B.Sc|M.Sc\"))\n",
    "            year_tag = school.find(\"span\", string=re.compile(r\"\\d{4}\"))\n",
    "            profile[\"education\"].append({\n",
    "                \"title\": school_name.get_text(strip=True) if school_name else None,\n",
    "                \"degree\": degree.get_text(strip=True) if degree else None,\n",
    "                \"start_year\": year_tag.get_text(strip=True) if year_tag else None\n",
    "            })\n",
    "\n",
    "    # Followers\n",
    "    followers_tag = soup.find(string=re.compile(r\"followers\"))\n",
    "    if followers_tag:\n",
    "        match = re.findall(r\"\\d[\\d,]*\", followers_tag)\n",
    "        if match:\n",
    "            profile[\"followers\"] = int(match[0].replace(\",\", \"\"))\n",
    "\n",
    "    # Connections\n",
    "    connections_tag = soup.find(string=re.compile(r\"connections\"))\n",
    "    if connections_tag:\n",
    "        match = re.findall(r\"\\d[\\d,]*\", connections_tag)\n",
    "        if match:\n",
    "            profile[\"connections\"] = int(match[0].replace(\",\", \"\"))\n",
    "\n",
    "    return profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "49c08eca-4b18-40b6-820f-403988447f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Name element not found — possibly not logged in or restricted profile.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': None,\n",
       " 'name': None,\n",
       " 'city': None,\n",
       " 'country_code': None,\n",
       " 'position': None,\n",
       " 'about': None,\n",
       " 'current_company': {'name': None,\n",
       "  'id': None,\n",
       "  'title': None,\n",
       "  'location': None},\n",
       " 'experience': [],\n",
       " 'url': 'https://www.linkedin.com/in/jai-soni-879764257/',\n",
       " 'education': [],\n",
       " 'avatar': None,\n",
       " 'certifications': [],\n",
       " 'followers': None,\n",
       " 'connections': None,\n",
       " 'projects': [],\n",
       " 'location': None,\n",
       " 'activity': []}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_url = \"https://www.linkedin.com/in/jai-soni-879764257/\"\n",
    "html = fetch_profile_html_with_cookies(profile_url)\n",
    "parse_linkedin_profile(html, profile_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66034fff-a0dc-4676-86dd-da646863144d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
